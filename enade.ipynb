{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import zipfile\n",
    "import rarfile\n",
    "import py7zr\n",
    "import magic\n",
    "import re\n",
    "import shutil\n",
    "import io\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.gov.br/inep/pt-br/acesso-a-informacao/dados-abertos/microdados/enade'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "content_core = soup.find(id=\"content-core\")\n",
    "links = content_core.find_all(\"a\")\n",
    "hrefs = [link.get(\"href\") for link in links if link.get(\"href\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Stream=True divide os dados(chunk) para aumentar a eficiência e auxiliar a memória\n",
    "###### iter_content é usado para binários(videos,imagens,zip)\n",
    "###### iter_lines é usaod para arquivos de texto(csv,log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_folders():\n",
    "    download_folder = \"downloads\"\n",
    "    extract_folder = \"extracted_files\"\n",
    "    transformed_parquet_files = \"transformed_parquet_files\"\n",
    "    os.makedirs(download_folder, exist_ok=True)\n",
    "    os.makedirs(extract_folder, exist_ok=True)\n",
    "    os.makedirs(transformed_parquet_files, exist_ok=True)\n",
    "    return download_folder, extract_folder, transformed_parquet_files\n",
    "\n",
    "download_folder, extract_folder, transformed_parquet_files = setup_folders()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baixar(url):    \n",
    "    \n",
    "    try:        \n",
    "        filename = os.path.join(download_folder, os.path.basename(url))  \n",
    "        response = requests.get(url, stream=True, timeout=30) \n",
    "\n",
    "        if response.status_code == 200:  #code status\n",
    "            with open(filename, \"wb\") as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024): \n",
    "                    f.write(chunk)\n",
    "            print(f\"Download concluído: {filename}\")\n",
    "        else:\n",
    "            print(f\"Erro ao baixar {url} (Status Code: {response.status_code})\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e: \n",
    "        print(f\" Erro ao baixar {url}: {e}\")\n",
    "\n",
    "for href in hrefs:  \n",
    "    baixar(href)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectar_tipo_arquivo(filename):\n",
    "    tipo = magic.Magic(mime=True).from_file(filename)\n",
    "    print(filename)\n",
    "\n",
    "    if \"zip\" in tipo:\n",
    "        return \"zip\"\n",
    "    elif \"rar\" in tipo:\n",
    "        return \"rar\"\n",
    "    elif \"7z\" in tipo or \"x-7z-compressed\" in tipo:\n",
    "        return \"7z\"\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair(filename, extract_folder):\n",
    "    try:\n",
    "        if not os.path.exists(filename):\n",
    "            raise FileNotFoundError(f\"Arquivo {filename} não encontrado.\")\n",
    "        \n",
    "        formato_real = detectar_tipo_arquivo(filename)\n",
    "        \n",
    "        if formato_real == \"zip\":\n",
    "            with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(extract_folder)\n",
    "            print(f\"Extraído para: {extract_folder}\")\n",
    "        \n",
    "        elif formato_real == \"rar\":\n",
    "            with rarfile.RarFile(filename) as rar_ref:\n",
    "                rar_ref.extractall(extract_folder)\n",
    "        \n",
    "        elif formato_real == \"7z\":\n",
    "            with py7zr.SevenZipFile(filename, \"r\") as sevenz_ref:\n",
    "                sevenz_ref.extractall(extract_folder)\n",
    "\n",
    "    except zipfile.BadZipFile as e:\n",
    "        print(f\"Erro: {e}\")\n",
    "    except rarfile.BadRarFile:\n",
    "        print(f\"Erro: O arquivo RAR {filename} está corrompido ou inválido.\")\n",
    "    except py7zr.Bad7zFile:\n",
    "        print(f\"Erro: O arquivo 7Z {filename} está corrompido ou inválido.\")\n",
    "    except PermissionError:\n",
    "        print(f\"Erro: Permissão negada para extrair arquivos em {extract_folder}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro inesperado ao extrair {filename}: {e}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for href in hrefs: \n",
    "    filename = os.path.join(download_folder, os.path.basename(href)) \n",
    "    extrair(filename, extract_folder) #só o rar que não funcionou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### criando estrutura de pastas do transformed_parquet_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home/951551482@adm.unifor.br/Documentos/microdados-enade/extracted_files\"\n",
    "base_destination = \"/home/951551482@adm.unifor.br/Documentos/microdados-enade/transformed_parquet_files\"\n",
    "\n",
    "for year in range(2013, 2022):\n",
    "\n",
    "    possible_folders = [\n",
    "        os.path.join(root_dir, f\"microdados_Enade_{year}_LGPD\", \"2.DADOS\"),\n",
    "        os.path.join(root_dir, f\"microdados_Enade_{year}_LGPD\", \"2. DADOS\")\n",
    "    ]\n",
    "\n",
    "    folder_path = next((folder for folder in possible_folders if os.path.exists(folder)), None)\n",
    "    if folder_path: #adm.unifor.br/Documentos/microdados-enade/extracted_files/microdados_Enade_2013_LGPD/2.DADOS\n",
    "        destination_folder = os.path.join(base_destination, f\"microdados_Enade_{year}_LGPD\") #@adm.unifor.br/Documentos/microdados-enade/transformed_parquet_files/microdados_Enade_2013_LGPD\n",
    "        os.makedirs(destination_folder, exist_ok=True)  # Garante que o diretório de destino exista\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.startswith(f\"microdados{year}_arq\") and file.endswith(\".txt\"): #microdados2013_arq1.txt\n",
    "                source_file = os.path.join(folder_path, file) #2@adm.unifor.br/Documentos/microdados-enade/extracted_files/microdados_Enade_2013_LGPD/2.DADOS/microdados2013_arq1.txt\n",
    "                parquet_file = os.path.join(destination_folder, f\"{os.path.splitext(file)[0]}.parquet\") #nome do arquivo parquet\n",
    "                #df\n",
    "                try:\n",
    "                    df = pd.read_csv(source_file, delimiter=';', dtype=str , low_memory=False)\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao ler o arquivo {source_file}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                table = pa.Table.from_pandas(df)\n",
    "\n",
    "                try:\n",
    "                    pq.write_table(table, parquet_file)\n",
    "                    print(f'Arquivo Parquet criado: {parquet_file}')\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao escrever o arquivo Parquet {parquet_file}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            \n",
    "               \n",
    "       \n",
    "\n",
    "                   \n",
    "               \n",
    "\n",
    "            \n",
    "                    \n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/home/951551482@adm.unifor.br/Documentos/microdados-enade/transformed_parquet_files\"\n",
    "dataframes = {}\n",
    "\n",
    "for root, dirs, files in os.walk(base_dir): #gera o nome das coisas em uma árvore de diretórios\n",
    "    for file in files:\n",
    "        if file.endswith(\".parquet\"):\n",
    "            sufixo = file.split('_')[-1].split('.')[0] #arq1 ....arq43\n",
    "            file_path = os.path.join(root, file)\n",
    "            df = pd.read_parquet(file_path)\n",
    "\n",
    "            \n",
    "\n",
    "# for sufixo, dfs in dataframes.items():\n",
    "#     print(sufixo)\n",
    "    # combined_df = pd.concat(dfs, ignore_index=True, axis=0)\n",
    "    # print(f\"DataFrame combinado para {sufixo}:\")\n",
    "    # print(combined_df)\n",
    "    # print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base_dir = \"/home/951551482@adm.unifor.br/Documentos/microdados-enade/transformed_parquet_files\"\n",
    "dataframes = {}\n",
    "\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".parquet\"):\n",
    "            sufixo = file.split('_')[-1].split('.')[0]  # Extrai o sufixo numérico (arq1, arq2, ..., arq43)\n",
    "            file_path = os.path.join(root, file)\n",
    "            df = pd.read_parquet(file_path)\n",
    "            \n",
    "            # Verifica se o sufixo já existe no dicionário\n",
    "            if sufixo in dataframes:\n",
    "                # Se existir, concatena o DataFrame atual com o existente\n",
    "                dataframes[sufixo] = pd.concat([dataframes[sufixo], df], ignore_index=True)\n",
    "            else:\n",
    "                # Se não existir, cria uma nova entrada no dicionário\n",
    "                dataframes[sufixo] = df\n",
    "\n",
    "# Agora, dataframes contém os DataFrames agrupados por sufixo\n",
    "# Por exemplo, dataframes['arq1'] contém todos os DataFrames concatenados com sufixo 'arq1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NU_ANO</th>\n",
       "      <th>CO_CURSO</th>\n",
       "      <th>QE_I04</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>2214</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>2214</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>11313</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>11313</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>11313</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3452045</th>\n",
       "      <td>2021</td>\n",
       "      <td>70985</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3452046</th>\n",
       "      <td>2021</td>\n",
       "      <td>70985</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3452047</th>\n",
       "      <td>2021</td>\n",
       "      <td>70985</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3452048</th>\n",
       "      <td>2021</td>\n",
       "      <td>70985</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3452049</th>\n",
       "      <td>2021</td>\n",
       "      <td>70985</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3452050 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        NU_ANO CO_CURSO QE_I04\n",
       "0         2013     2214      A\n",
       "1         2013     2214      A\n",
       "2         2013    11313      A\n",
       "3         2013    11313      A\n",
       "4         2013    11313      A\n",
       "...        ...      ...    ...\n",
       "3452045   2021    70985   None\n",
       "3452046   2021    70985   None\n",
       "3452047   2021    70985   None\n",
       "3452048   2021    70985   None\n",
       "3452049   2021    70985   None\n",
       "\n",
       "[3452050 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['arq10']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
